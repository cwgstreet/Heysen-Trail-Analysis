#!/usr/bin/env python3
# -*- coding: utf-8 -*-

  
# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
# Module name: extract_data.py
# Purpose:     Extract Heysen Trail trip data from rambl.com html
# 
# Notes:
#
# Copyright:   2019, release under GPL3. See LICENSE file for details
#              Carl W Greenstreet <cwgstreet@gmail.com>
# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^


# 1. Import standard libraries

#import libraries
import datetime   # manage date time formats
import re       # regular expressions to extract strings
import time       # time to allow sleep pause for JS to catch up

# import HT-analysis modules
import debug  #TODO:  remove this once module is working

#import third party libary specific imports
import bs4      # beautiful soup 4 library to parse website
import lxml     # lxml to parse website
import requests # requests to get http


# 2. Capture web page

# get JS generated dynamic HTML page
JS_dynamic_HTML = get_dynamic_HTML(target_url)

# Parse  JS_dynamic_HTML  variable, and store it in Beautiful Soup format
soup_JS_dynamic = bs4.BeautifulSoup(JS_dynamic_HTML, "lxml")

# use beautiful soup function prettify to display page
#print (soup_JS_dynamic.prettify()) # comment out as not needed once captured to text file

Title = soup_JS_dynamic.h1
Title = str(Title)
title = Title[Title.find("h1>")+3:Title.find("</h1>")]
print ("\nTitle =",title)
print ("\tTitle type:",(type(title))) #debug

#specify the target url  URL in format of webpage/web/mymap/trip/user_id/trip_id
target_url = "https://www.ramblr.com/web/mymap/trip/478170/1510167/"

#Query the website and return the html to the variable 'page'
page = requests.get(target_url)

# 2b. Capture inner html dynamically generated by Java Script
options = webdriver.ChromeOptions()
options.headless = True

browser_path = r"/Applications/chromedriver"
browser = webdriver.Chrome(executable_path=browser_path,
							options=options) 

browser.get(target_url) #navigate to the page

innerHTML = browser.execute_script("return document.body.innerHTML") #returns the inner HTML as a string

# Parse  innerHTML  variable, and store it in Beautiful Soup format
soup_inner = bs4.BeautifulSoup(innerHTML, "lxml")
soup = bs4.BeautifulSoup(page.text, features = "lxml")
#print("soup type:", type(soup))
#print("soup_innerHTML type:", type(soup_inner))

#use beautiful soup function prettify to display page
#  Output to file did not work:   print (soup.prettify(), file=open("output.txt", "a"))
#print (soup.prettify()) # comment out as not needed once catured to text file
#print (soup_inner.prettify()) # comment out as not needed once catured to text file


# 3. Extract data from Trail Journal web page

#---------------------------------------------------------------------
# Variable of interest:
# Day, Date, Start location, End Location, Council, distnce, total duration,
#   active duration, paused duration, avg speed, highest point, total ascent,
#   difficulty, rest day?, tent, hut, bed

print('++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++')  #make it easier to find start of output


#------- Title (Title)---------
Title = soup.h1
#print(type(Title))   # debug, returns <class 'bs4.element.Tag'>  from BeauifulSoup
#print(Title)         # debug
Title = str(Title)   # convests Title into string
# print('Title=',Title)    #debug test, returns Day= [6]
# print ("\tTitle type:",(type(Title)))

#------- Day (Day)---------
Day=[int(s) for s in re.findall(r'\b\d+\b', Title)]
print('Day=',Day)    #debug test, returns Day= [6]
print ("\tDay type:",(type(Day)))

#------- Start Location (Start_location) ---------
keyword = " - "
after_keyword = Title.partition(keyword)
#print(after_keyword)        #debug 
#print("after_keyword type = ",type(after_keyword))  #debug 
keyword2 = ' to '           # split walk title into tuple at keyword
start_stop = after_keyword[2].partition(keyword2)
#print("start_stop=",start_stop)  #debug, prints tuple contents
Start_location=start_stop[0]     #assigns start location to first tuple element 
print('\nStart Location is',Start_location) #debug
print ("\tStart Location type:",(type(Start_location))) #debug

#------- Stop Location (Stop_location) ---------
keyword3 = '<'  #change split keyword to further seperate third tuple element
start_stop2 = start_stop[2].partition(keyword3)
Stop_location=start_stop2[0]
print('\nStop Location is',Stop_location)   #debug
print ("\tStop Location type:",(type(Stop_location))) #debug

#------- Title (title) ---------
title=Title[Title.find("h1>")+3:Title.find("</h1>")]
print ("\nTitle =",title)
print ("\tTitle type:",(type(title))) #debug

#-------Council (council)) ---------
location = soup.find('div', class_ ="content_addr")
#print(type(location))       #debug
#print("Location=",location)  #debug
location = str(location)     # convests location into string
print("\nLocation=",location)  #debug
#print(type(location))       #debug
#<div class="content_addr">Alexandrina Council, South Australia, Australia</div>
council=location[location.find(">")+1:location.find(",")]
print ("\nCouncil =",council)
print ("\tCouncil type:",(type(council))) #debug

#-------Recording Date (date) ---------
recording_date = soup.find('div', class_ ="content_recoding_time")
#print(type(recording_date))

recording_date = str(recording_date)     # convests date into string
print("\nRecording date=",recording_date)  #debug
print ("\tRecording date:",(type(recording_date))) #debug

date_timestamp=recording_date[recording_date.find(":")+2:recording_date.find(" <")-5]
print ("Recording timestamp =",date_timestamp)

recording_date_time_obj = datetime.datetime.strptime(date_timestamp, '%b %d, %Y %I:%M %p')

print('\tDate:', recording_date_time_obj.date())   #debug
print('\tTime:', recording_date_time_obj.time())   #debug
print('\tDate-time:', recording_date_time_obj)     #debug


#print('\n############################################################')  #make it easier to find this section in terminal output

#------- Extract all Durations (durations) ---------
durations = soup_inner.find_all('li', class_ ="aft")
durations = str(durations)
#print("Durations  =",durations)  #debug
#print ("\tDurations type:",(type(durations)))
durations = re.findall("\d+[h]\s\d+[m]\s\d+[s]", durations) 
print("\nDuration =",durations)  #debug
print ("\tDuration type:",(type(durations)))

#-------Total Duration (total_duration) ---------
total_duration=durations[0]
print("\nTotal Duration =",total_duration)  #debug
print ("\tTotal Duration type:",(type(total_duration)))

total_duration_time_obj = datetime.datetime.strptime(total_duration, '%Hh %Mm %Ss')   
print('Total Time:', total_duration_time_obj.time())   #debug

#-------Active Duration (paused_duration) ---------
active_duration=durations[1]
print("\nActive Duration =",active_duration)  #debug
print ("\tActive Duration type:",(type(active_duration)))

active_duration_time_obj = datetime.datetime.strptime(active_duration, '%Hh %Mm %Ss')   
print('Active Time:', active_duration_time_obj.time())   #debug

#-------Paused Duration (paused_duration) ---------
paused_duration=durations[2]
print("\nPaused Duration =",paused_duration)  #debug
print ("\tPaused Duration type:",(type(paused_duration)))

paused_duration_time_obj = datetime.datetime.strptime(paused_duration, '%Hh %Mm %Ss')   
print('Paused Time:', paused_duration_time_obj.time())   #debug

#print('############################################################')  #make it easier to find this section in terminal output

#------- Distance (distance) ---------
distance = soup.find('div', class_ ="content_distance")
#print(type(distance))        #debug
#print("distance=",distance)  #debug
distance = str(distance)   # convests distance into string
#print(type(distance))        #debug
distance=[float(s) for s in re.findall(r'\d+\.\d+', distance)]
print('\nDistance =',distance)    #debug test, returns Distance = [30.8]
print ("\tDistance type:",(type(distance)))

#------- Total Ascent (total_ascent)---------
total_ascent = soup.find('div', class_ ="content_total_ascent")
#print(type(total_ascent))
#print("Total Ascent=",total_ascent)  #debug
total_ascent = str(total_ascent)   # convests total_ascent into string
#print(type(total_ascent))        #debug
total_ascent=[int(s) for s in re.findall(r'\b\d+\b', total_ascent)]
print('\nTotal Ascent =',total_ascent)    #debug test, returns Total Ascent = [525]
print ("\tTotal Ascent type:",(type(total_ascent)))

#------- Highest Point (highest_point)---------
highest_point = soup.find('div', class_ ="content_highest_point")
#print(type(highest_point))
#print("Highest Point =",highest_point)  #debug
highest_point = str(highest_point)   # convests highest_point into string
#print(type(highest_point))        #debug
highest_point=[int(s) for s in re.findall(r'\b\d+\b', highest_point)]
print('\nHighest Point =',highest_point)    #debug test, returns Highest Point = [389]
print ("\tHighest Point type:",(type(highest_point)))

#------- Average Speed (avg_spped)---------
avg_speed = soup.find('div', class_ ="content_avg_speed")
#print(type(avg_speed))
#print("Average Speed =",avg_speed)  #debug
avg_speed = str(avg_speed)   # convests avg_speed into string
#print(type(avg_speed))        #debug
avg_speed=[float(s) for s in re.findall(r'\d+\.\d+', avg_speed)]
print('\nAverage Speed =',avg_speed)    #debug test, returns Average Speed = [30.8]
print ("\tAverage Speed type:",(type(avg_speed)))


#print('############################################################')  #make it easier to find this section in terminal output






